{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import regex\n",
    "import pathlib\n",
    "from functools import reduce\n",
    "import csv\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import ngrams, pad_sequence\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, PlaintextCorpusReader\n",
    "from nltk.tokenize import regexp_tokenize, sent_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df203ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../assets/cyberbullying_tweets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9088e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tweet_text\"].apply(lambda n: len(n.split())).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b428f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null values\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f473e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cyberbullying_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2999fc8a-1ded-4336-907e-5b3d66aaa72f",
   "metadata": {},
   "source": [
    "## Sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_email_pattern = r\"\\S+@\\S+\\.\\S+\"\n",
    "normal_email_pattern = r\"[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*@(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\"\n",
    "\n",
    "simple_phone_pattern = r\"\\\\+?[1-9][0-9]{7,14}\"\n",
    "normal_phone_pattern = r\"\\+?\\d{1,4}?[-.\\s]?\\(?\\d{1,3}?\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}\"\n",
    "\n",
    "normal_url_pattern = r\"(?:https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n",
    "normal_url_pattern_v2 = r\"(?:https:\\/\\/www\\.|http:\\/\\/www\\.|https:\\/\\/|http:\\/\\/)?[a-zA-Z]{2,}(?:\\.[a-zA-Z]{2,})(?:\\.[a-zA-Z]{2,})?\\/[a-zA-Z0-9]{2,}|(?:(?:https:\\/\\/www\\.|http:\\/\\/www\\.|https:\\/\\/|http:\\/\\/)?[a-zA-Z]{2,}(?:\\.[a-zA-Z]{2,})(?:\\.[a-zA-Z]{2,})?)|(?:https:\\/\\/www\\.|http:\\/\\/www\\.|https:\\/\\/|http:\\/\\/)?[a-zA-Z0-9]{2,}\\.[a-zA-Z0-9]{2,}\\.[a-zA-Z0-9]{2,}(\\.[a-zA-Z0-9]{2,})?\"\n",
    "# https://ihateregex.io/expr/phone/\n",
    "#normal_phone_pattern = r\"[\\+]?[(]?[0-9]{3}[)]?[-\\s\\.]?[0-9]{3}[-\\s\\.]?[0-9]{4,6}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57bc2b-c0ba-490c-a044-b9e6a00878d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
    "digits = \"([0-9])\"\n",
    "multiple_dots = r'\\.{2,}'\n",
    "newlines = \"\\n+\"\n",
    "\n",
    "def split_into_sentences(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split the text into sentences.\n",
    "\n",
    "    If the text contains substrings \"<prd>\" or \"<stop>\", they would lead \n",
    "    to incorrect splitting because they are used as markers for splitting.\n",
    "\n",
    "    :param text: text to be split into sentences\n",
    "    :type text: str\n",
    "\n",
    "    :return: list of sentences\n",
    "    :rtype: list[str]\n",
    "    \"\"\"\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = re.sub(\"^\\s+\", \" \", text)\n",
    "    \n",
    "    email_iter = re.finditer(normal_email_pattern, text)\n",
    "    for m in email_iter:\n",
    "        text = text.replace(m.group(), m.group().replace(\".\", \"<prd>\"))\n",
    "\n",
    "    phone_iter = re.finditer(normal_phone_pattern, text)\n",
    "    for m in phone_iter:\n",
    "        text = text.replace(m.group(), m.group().replace(\".\", \"<prd>\"))\n",
    "\n",
    "    url_iter = re.finditer(normal_url_pattern, text)\n",
    "    for m in url_iter:\n",
    "        text = text.replace(m.group(), m.group().replace(\".\", \"<prd>\"))\n",
    "\n",
    "    text = re.sub(r\"(\\,|\\:|\\;)*((\\?|\\!|\\.)+)(\\,|\\:|\\;)*\", \"\\\\2\", text)\n",
    "    text = re.sub(r\"(\\.+)[\\?\\!\\.]*\", \"\\\\1<stop>\", text)\n",
    "    text = re.sub(r\"((\\?|\\!)+)[\\?\\!\\.]*\", \"\\\\1<stop>\", text)\n",
    "    text = re.sub(r\"((\\?+\\!)|(\\!+\\?))[\\?\\!\\.]*\", \"?!<stop>\", text)\n",
    "        \n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    text = re.sub(multiple_dots, lambda match: \"<prd>\" * len(match.group(0)) + \"<stop>\", text)\n",
    "\n",
    "    text = re.sub(newlines, \"<stop>\", text)\n",
    "    \n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms + \" \" + starters, \"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \" + suffixes + \"[.] \" + starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \" + suffixes + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"(?|!|?!)\",\"\\\\1<stop>\")\n",
    "\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    \n",
    "    text = re.sub(\"(\\s*<stop>\\s*){2,}\", \"<stop>\", text)\n",
    "    \n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    if sentences and not sentences[-1]: sentences = sentences[:-1]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97ac170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text, method=\"custom\") -> list[str]:\n",
    "    if method == \"nltk\":\n",
    "        return sent_tokenize(text)\n",
    "    elif method == \"custom\":\n",
    "        return split_into_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ddcca2-5bc2-4477-89d2-2f9775ce5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentences\"] = df[\"tweet_text\"].apply(split_into_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb015b-3017-4bb2-9e01-64b671cc8b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentences_count\"] = df[\"sentences\"].apply(len)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9bda7-ab99-4136-a6ed-d398bc9f1098",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTDATED FUNCTION\n",
    "\n",
    "def tokenize_text(text, remove_hashtags=True):\n",
    "    # Replace newline and carriage return with space\n",
    "    text = re.sub(r'\\r|\\n', ' ', text)\n",
    "    # Remove urls\n",
    "    text = re.sub(r'(?:https?\\://|www\\.)\\S+', '', text)\n",
    "    # Remove emails\n",
    "    text = re.sub(normal_email_pattern, '', text)\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'\\@\\S+', ' ', text)\n",
    "    if remove_hashtags:\n",
    "        # Remove hashtags\n",
    "        text = re.sub(r'#\\S+', ' ', text)\n",
    "    else:\n",
    "        # Remove the # symbol from hashtags\n",
    "        text = re.sub(r'#(?:[\\w-]+)', r'\\1', text).strip()\n",
    "    \n",
    "    # Remove simple emotiocons\n",
    "    text = re.sub(r'(:|;|=)(-|_)?(\\)|\\(|D|P)', '', text)\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', '', text)\n",
    "    # Remove numbers\n",
    "    #     text = re.sub(r'\\d+', '', text)\n",
    "    # Replace multispaces with one\n",
    "    text = re.sub(r\"\\s\\s+\", \" \", text)\n",
    "    # Delete repeated punctuation\n",
    "    text = re.sub(r'[\\?\\.\\!]+(?=[\\?\\.\\!])', '', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    words = text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f181a-780a-4e98-87d0-42557cd62555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_tokenize_pattern():\n",
    "    words = r\"\\w+(?:-\\w+)*(?:\\'(?:s|re|m|t))?\"\n",
    "    mentions = r'@[\\w\\_\\.]+'\n",
    "    hashtags = r'#[^ !@#$%^&*(),.?\":{}|<>]*'\n",
    "    emotiocons = r'(?:(?::|;|=)(?:-|_)?(?:\\)|\\(|D|P))|(?:[-*]_[-*])'\n",
    "    numbers = r\"[0-9]+\\.?[0-9]*[\\%\\$]?\"\n",
    "\n",
    "    ending_signs = r'(?:\\?|\\.|\\.\\.\\.|\\!|\\?\\!|\\!\\?)$'\n",
    "\n",
    "    return r'|'.join([\n",
    "        normal_url_pattern, normal_email_pattern, normal_phone_pattern, mentions, hashtags, emotiocons, numbers, words, ending_signs\n",
    "    ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25b148-afaa-43b0-bd0d-7e24fd55a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    tokenize_pattern = get_text_tokenize_pattern()\n",
    "\n",
    "    return [\n",
    "        token for token in [\n",
    "            regexp_tokenize(sentence, tokenize_pattern) for sentence in sentences\n",
    "        ] if len(token) > 0 and not (len(token) == 1 and token[0] in [\"?\", \"!\", \".\", \"...\", \"?!\", \"!?\"])\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529161c-c2af-4f9a-af01-611f936f2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokenized\"] = df[\"sentences\"].apply(tokenize_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba1149-8e7c-40ee-9e9c-2ee3a422fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens_count\"] = df[\"tokenized\"].apply(lambda x: [len(i) for i in x])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c751f75e-aa29-44c9-9f56-13282f111921",
   "metadata": {},
   "source": [
    "## Stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b64abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stem_text(text):\n",
    "    return [\n",
    "        [stemmer.stem(word) for word in sentence] for sentence in text\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed8745-1bea-4ef7-b187-126cd9c15823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"stem\"] = df[\"tokenized\"].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723f7db-9ef7-4d6b-bf2a-420c8e3b5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27295ce-eced-4777-9767-911ef14d6b56",
   "metadata": {},
   "source": [
    "## Lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_text(text):\n",
    "    return [\n",
    "        [lemmatizer.lemmatize(word) for word in sentence] for sentence in text\n",
    "    ]\n",
    "#     words = nltk.word_tokenize(text)\n",
    "#     return \" \".join([lematizer.lemmatize(word) for word in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb388f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemmatized\"] = df[\"tokenized\"].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b85555-306a-4553-8097-c590f4bc3904",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238bafe-9961-4349-831e-f064fa39bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cyberbullying_type in df[\"cyberbullying_type\"].unique():\n",
    "    for subset in [\"train\", \"test\"]:\n",
    "        pathlib.Path(f'../assets/annotated-corpus/{subset}/{cyberbullying_type}').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85482d6e-e7f9-42fb-86fe-2b5ab4f79e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[['tokenized', 'stem', 'lemmatized']],\n",
    "    df[\"cyberbullying_type\"],\n",
    "    train_size=0.75,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c21e931-39f0-4d33-99e4-7bc37b215093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(X: pd.DataFrame, y: pd.Series, subset):\n",
    "    for i, ((_, (tokens_s, stems_s, lemms_s)), cyberbullying_type) in enumerate(zip(X.iterrows(), y)):\n",
    "        with open(f'../assets/annotated-corpus/{subset}/{cyberbullying_type}/{i:03d}.tsv', 'w', newline='') as tsvfile:\n",
    "            writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n",
    "            for tokens, stems, lemms in zip(tokens_s, stems_s, lemms_s):\n",
    "                for token, stem, lemm in zip(tokens, stems, lemms):\n",
    "                    writer.writerow([token, stem, lemm])\n",
    "                writer.writerow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2cbdd-b942-4260-ab98-039ad4e052f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(X_test, y_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1816174e-11eb-4a55-a991-7c06e402884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(X_train, y_train, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863ffe90-727f-4921-b355-c0b35582586b",
   "metadata": {},
   "source": [
    "# Lab 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f03bd-4b57-42ec-a219-3336d6b03cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ec3ba-42c9-47c3-97d0-7d896356582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af044c4-2a10-43eb-aa7d-bbcdfd7e3a2c",
   "metadata": {},
   "source": [
    "## Пункты 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db1208f-cc90-474d-b969-dffae16aba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_tokenize_pattern_v2():\n",
    "    words = r\"\\w+(?:-\\w+)*(?:\\'(?:s|re|m|t))?\"\n",
    "    mentions = r'@[\\w\\_\\.]+'\n",
    "    hashtags = r'#[^ !@#$%^&*(),.?\":{}|<>]*'\n",
    "    emotiocons = r'(?:(?::|;|=)(?:-|_)?(?:\\)|\\(|D|P))|(?:[-*]_[-*])'\n",
    "\n",
    "    numbers = r\"[0-9]+\\.?[0-9]*[\\%\\$]?\"\n",
    "\n",
    "    return r'|'.join([\n",
    "        normal_url_pattern, normal_email_pattern, normal_phone_pattern, mentions, hashtags, emotiocons, numbers, words\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11231002-426d-449c-9086-77269e92cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences_v2(sentences):\n",
    "    tokenize_pattern = get_text_tokenize_pattern_v2()\n",
    "\n",
    "    tokenized_sentences = list()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentence = list()\n",
    "        tokens = regexp_tokenize(sentence, tokenize_pattern)\n",
    "        for token in tokens:\n",
    "            token_low = token.strip().lower()\n",
    "            if len(token_low) > 0 and not (token_low in stop_words):\n",
    "                tokenized_sentence.append(token)\n",
    "        if len(tokenized_sentence) > 0:\n",
    "            tokenized_sentences.append(tokenized_sentence)\n",
    "\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e852c5e5-37ab-44f5-b607-f31a2f7b1284",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[[\"tweet_text\", \"cyberbullying_type\", \"sentences\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150c25eb-d79a-4f70-ac49-19a86bac363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"tokenized\"] = df_test[\"sentences\"].apply(tokenize_sentences_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab23240-d728-4d90-b9d2-631049809a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"tokens_count\"] = df_test[\"tokenized\"].apply(lambda x: [len(i) for i in x])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b07a31b-f464-4c48-88ed-173ed1a83d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"lemmatized\"] = df_test[\"tokenized\"].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a2d808-93ca-4b73-9884-60edfb41393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe230da-a8ce-42e3-a907-e023363b79c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test = [sentence for row in df_test[\"lemmatized\"].to_list() for sentence in row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f863e1da-3c60-499f-96ba-93bffe2d2e01",
   "metadata": {},
   "source": [
    "# Пункт 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce5ab48-5b78-45a0-ad97-84bc22930b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "trigrams = list()\n",
    "\n",
    "for sentence in sentences_test:\n",
    "    for i in range(len(sentence) - 2):\n",
    "        trigrams.append(tuple(sentence[i:i + 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021eb1d8-31bd-48d3-82a9-7934438e4577",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4d4ad-450a-448a-a722-ab6a474efad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_counter = Counter(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005aef04-d1c9-4e70-81d5-7371dd80f160",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_tokens = [ngrams(sent, ngram_size) for sent in sentences_test]\n",
    "ngram_counts = Counter([gram for tokens in ngram_tokens for gram in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361dd78-8580-462c-a569-35480d084cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(sentences, n=3):\n",
    "    ngrams = Counter()\n",
    "\n",
    "    for sentence in sentences_test:\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            ngrams[tuple(sentence[i:i + n])] += 1\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a056de8-8f12-43be-9d9e-079d2e2bf679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigrams_cnt = create_ngrams(sentences_test, 1)\n",
    "trigrams_cnt = create_ngrams(sentences_test, 3)\n",
    "unigrams_cnt = create_ngrams(list(trigrams_cnt.keys()), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324917a-1bff-4cd2-962a-3d0e7a021b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25bf7e-015e-43d7-9a13-0514198f59b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ngrams(sentences_test, 1).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d25d5-0512-47eb-b197-e202557e5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(trigrams_cnt.keys())), len(trigrams_cnt), len(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd5741-6a57-42dd-8535-b4f8fde5b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(ngrams([word for sentence in sentences_test for word in sentence], 3, pad_right=True, right_pad_symbol=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739f717-9ac4-4a55-a219-87020c7e32cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ngrams_for_trigrams(trigrams):\n",
    "    word_fd = FreqDist()\n",
    "    wildcard_fd = FreqDist()\n",
    "    bigram_fd = FreqDist()\n",
    "    ngram_fd = FreqDist()\n",
    "    for window in trigrams:\n",
    "        w1 = window[0]\n",
    "        if w1 is None:\n",
    "            continue\n",
    "        for w2, w3 in itertools.combinations(window[1:], 2):\n",
    "            word_fd[w1] += 1\n",
    "            if w2 is None:\n",
    "                continue\n",
    "            bigram_fd[(w1, w2)] += 1\n",
    "            if w3 is None:\n",
    "                continue\n",
    "            wildcard_fd[(w1, w3)] += 1\n",
    "            ngram_fd[(w1, w2, w3)] += 1\n",
    "    return word_fd, bigram_fd, wildcard_fd, ngram_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5b9b5-eba7-460e-8ebc-a784f57f20b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ngrams_for_trigrams_v2(trigrams):\n",
    "    word_fd = Counter()\n",
    "    wildcard_fd = Counter()\n",
    "    bigram_fd = Counter()\n",
    "    ngram_fd = Counter()\n",
    "    for window in trigrams:\n",
    "        w1 = window[0]\n",
    "        if w1 is None:\n",
    "            continue\n",
    "        for w2, w3 in itertools.combinations(window[1:], 2):\n",
    "            word_fd[w1] += 1\n",
    "            if w2 is None:\n",
    "                continue\n",
    "            bigram_fd[(w1, w2)] += 1\n",
    "            if w3 is None:\n",
    "                continue\n",
    "            wildcard_fd[(w1, w3)] += 1\n",
    "            ngram_fd[(w1, w2, w3)] += 1\n",
    "    return word_fd, bigram_fd, wildcard_fd, ngram_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020449f3-38d4-49bd-930b-7fa219c544bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = list()\n",
    "for sentence in sentences_test:\n",
    "    for i in range(len(sentence) - 2):\n",
    "        trigrams.append(tuple(sentence[i:i + 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabbb99a-76e7-4e45-be55-fb528e692b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "strange_trigrams = list()\n",
    "\n",
    "for sentence in sentences_test:\n",
    "    snt_len = len(sentence)\n",
    "    for i in range(len(sentence)):\n",
    "        t = [sentence[i]]\n",
    "        if i+1 >= snt_len:\n",
    "            t.append(None)\n",
    "        else:\n",
    "            t.append(sentence[i+1])\n",
    "\n",
    "        if i+2 >= snt_len:\n",
    "            t.append(None)\n",
    "        else:\n",
    "            t.append(sentence[i+2])\n",
    "        \n",
    "        strange_trigrams.append(tuple(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a0571-7ec9-465d-94e3-0a299d22ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "strange_trigrams_cnt = Counter()\n",
    "\n",
    "for sentence in sentences_test:\n",
    "    snt_len = len(sentence)\n",
    "    for i in range(len(sentence)):\n",
    "        t = [sentence[i]]\n",
    "        if i+1 >= snt_len:\n",
    "            t.append(None)\n",
    "        else:\n",
    "            t.append(sentence[i+1])\n",
    "\n",
    "        if i+2 >= snt_len:\n",
    "            t.append(None)\n",
    "        else:\n",
    "            t.append(sentence[i+2])\n",
    "        \n",
    "        strange_trigrams_cnt[tuple(t)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68d711-8b2e-4eb5-afea-170bf79ded2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fd_v1, bigram_fd_v1, wildcard_fd_v1, ngram_fd_v1 = prepare_ngrams_for_trigrams(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fb6fe-fea7-4121-8f1e-32066f3d6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fd_v2, bigram_fd_v2, wildcard_fd_v2, ngram_fd_v2 = prepare_ngrams_for_trigrams_v2(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d62154-6703-48d4-a714-3f070c9f36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fd_v1_s, bigram_fd_v1_s, wildcard_fd_v1_s, ngram_fd_v1_s = prepare_ngrams_for_trigrams(strange_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3853476-d843-4711-a27c-cee2505fc397",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fd_v2_s, bigram_fd_v2_s, wildcard_fd_v2_s, ngram_fd_v2_s = prepare_ngrams_for_trigrams_v2(strange_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396aaf90-2004-4bdb-bba7-6361fb4b2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fd_v2_s2, bigram_fd_v2_s2, wildcard_fd_v2_s2, ngram_fd_v2_s2 = prepare_ngrams_for_trigrams_v2(list(Counter(trigrams).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7884b0c-8184-424f-a651-e9ec44b3cfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7075a2d-5c92-4da9-9ed6-db7cf6ad57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(ngram_fd_v2)[('bullied', 'high', 'school')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89709b4-89b2-4550-939e-dd9f37ded1c6",
   "metadata": {},
   "source": [
    "## Используя NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558d5dd-fa55-4807-9cab-76f0e95d4e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Словарь для подсчета триграмм\n",
    "trigram_counter_nltk = Counter()\n",
    "\n",
    "for sentence in sentences_test:\n",
    "    # Генерируем триграммы и преобразуем их в строки, сразу подсчитываем их\n",
    "    trigram_counter_nltk.update(\" \".join(ngram) for ngram in ngrams(sentence, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347d829-e312-4e12-9115-f59c02d49e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_counter_nltk.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eaf08b-5e70-41b3-b600-33d88bfb400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_cnt.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e726b-a903-4222-8efb-25bbdd216dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Находим топ 30 самых популярных триграмм\n",
    "top_30_trigrams = trigrams_cnt.most_common(30)\n",
    "\n",
    "# Распаковываем данные\n",
    "trigrams, counts = zip(*top_30_trigrams)\n",
    "\n",
    "# Создаем построение\n",
    "plt.figure(figsize=(10, 8), dpi=180)\n",
    "plt.barh(list(map(lambda x: \" \".join(x), trigrams)), counts, color='skyblue')\n",
    "plt.xlabel('Частота')\n",
    "plt.ylabel('Триграмма')\n",
    "plt.title('Топ-30 самых популярных триграмм')\n",
    "plt.gca().invert_yaxis()  # перевернуть ось Y, чтобы самая частая триграмма была наверху\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075dc021-542a-4230-91d0-acfb08162df0",
   "metadata": {},
   "source": [
    "# Пункт 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf8a32-1759-44f8-aaa6-7a777b8b7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_score(word, colocates, ngrams, unigrams, N, n=3, t=None):\n",
    "    assert n > 1\n",
    "    if ngrams[(word,) + colocates] == 0:\n",
    "        return 0\n",
    "    m = 1\n",
    "    if t is not None:\n",
    "        for colocate in (word,) + colocates:\n",
    "            m *= t.count(colocate)\n",
    "    else:\n",
    "        for colocate in (word,) + colocates:\n",
    "            m *= unigrams[(colocate,)]\n",
    "    return math.log2(ngrams[(word,) + colocates] * (N ** (n - 1)) / m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c12bb70-1bf1-4b97-b6cf-9a2d46242199",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_score_dict = dict()\n",
    "words = [word for sentence in sentences_test for word in sentence]\n",
    "N = len(words)\n",
    "for trigram in sorted(trigrams_counter.items(), key=lambda x:x[1], reverse=True)[:100]:\n",
    "    score = mi_score(trigram[0][0], trigram[0][1:], trigrams_counter, unigrams, N, t=words)\n",
    "    mi_score_dict[trigram[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e411f36-ac8b-40a0-867a-ace01c48df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "list({k: v for k, v in sorted(mi_score_dict.items(), key=lambda item: item[1], reverse=True)})[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ec6f0-b087-40d9-aca4-095e287de42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe4267-9b5a-4299-b5d8-f49d822fe23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list({k: v for k, v in sorted(mi_score_dict.items(), key=lambda item: item[1], reverse=True)})[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683221ed-1f25-4c12-992f-c1a81a991136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d918b-6491-482f-bfe9-5498935c6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_cnt = create_ngrams(sentences_test, 3)\n",
    "unigrams_cnt = create_ngrams(list(trigrams_cnt.keys()), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25a3c4-c7c4-4b7b-ad42-a662159f82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_score_dict = dict()\n",
    "words = [word for sentence in sentences_test for word in sentence]\n",
    "N = len(words)\n",
    "for trigram in sorted(trigrams_cnt.items(), key=lambda x:x[1], reverse=True):\n",
    "    score = mi_score(trigram[0][0], trigram[0][1:], trigrams_cnt, unigrams_cnt, N)\n",
    "    mi_score_dict[trigram[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c2ec6-2177-4f1a-9e5e-2c4589549700",
   "metadata": {},
   "outputs": [],
   "source": [
    "list({k: v for k, v in sorted(mi_score_dict.items(), key=lambda item: item[1], reverse=True)})[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d16255-fbe4-457e-91ea-983419d564a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_score_v2(word, colocates, ngrams, unigrams, N, n=3, t=None):\n",
    "    assert n > 1\n",
    "    if ngrams[(word,) + colocates] == 0:\n",
    "        return 0\n",
    "    m = 1\n",
    "    if t is not None:\n",
    "        for colocate in (word,) + colocates:\n",
    "            m *= t.count(colocate)\n",
    "    else:\n",
    "        for colocate in (word,) + colocates:\n",
    "            m *= unigrams[colocate]\n",
    "    return math.log2(ngrams[(word,) + colocates] * (N ** (n - 1)) / m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb7389-7faa-41bd-b294-f21e89dc2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_score_dict = dict()\n",
    "words = [word for sentence in sentences_test for word in sentence]\n",
    "N = sum(word_fd_v2_s.values())\n",
    "\n",
    "for trigram in ngram_fd_v2_s.keys():\n",
    "    score = mi_score_v2(trigram[0], trigram[1:], ngram_fd_v2_s, word_fd_v2_s, N)\n",
    "    mi_score_dict[trigram] = score\n",
    "    # print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b1bae-760a-4a3b-8cea-a26fd8fd7587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(mi_score_dict.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6084c-065c-44f0-aee7-f670a03305e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_score_dict[('#10ThousandStepsAgain', '#KillinIt', 'https://t.co/gwoaRbVSUw')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989b8a0-1add-46fe-9a99-214a2a8c24fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list({k: v for k, v in sorted(mi_score_dict.items(), key=lambda item: item[1], reverse=True)})[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99721a99-b868-44bf-b4fd-e08d2775f5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6865f9-a4ad-42a9-8b76-9abec748779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi_v2(*marginals):\n",
    "    return math.log2(marginals[0] * marginals[-1] ** (3 - 1)) - math.log2(reduce(lambda x, y: x * y, marginals[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277867c-1214-4210-946c-f21370532898",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_score_dict_2 = dict()\n",
    "n_all = sum(word_fd_v2_s.values())\n",
    "for trigram in sorted(ngram_fd_v2_s.items(), key=lambda x:x[1], reverse=True):\n",
    "    w1_, w2_, w3_ = trigram[0]\n",
    "    n_iii = ngram_fd_v2_s[(w1_, w2_, w3_)]\n",
    "    if not n_iii:\n",
    "        continue\n",
    "    n_iix = bigram_fd_v2_s[(w1_, w2_)]\n",
    "    n_ixi = wildcard_fd_v2_s[(w1_, w3_)]\n",
    "    n_xii = bigram_fd_v2_s[(w2_, w3_)]\n",
    "    n_ixx = word_fd_v2_s[w1_]\n",
    "    n_xix = word_fd_v2_s[w2_]\n",
    "    n_xxi = word_fd_v2_s[w3_]\n",
    "    score = pmi_v2(n_iii, (n_iix, n_ixi, n_xii), (n_ixx, n_xix, n_xxi), n_all)\n",
    "    \n",
    "    mi_score_dict_2[trigram[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ac3f73-cf2e-401b-ae20-c48378999acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449bbbec-d48d-4f84-bd55-36ad333887bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "list({k: v for k, v in sorted(mi_score_dict_2.items(), key=lambda item: item[1], reverse=True)})[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25739c24-1018-4f40-b19f-1356c4c5ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_score_dict_2 = dict()\n",
    "n_all = sum(word_fd_v2_s2.values())\n",
    "for trigram in sorted(ngram_fd_v2_s2.items(), key=lambda x:x[1], reverse=True):\n",
    "    w1, w2, w3 = trigram[0]\n",
    "    n_iii = ngram_fd_v2_s2[(w1, w2, w3)]\n",
    "    if not n_iii:\n",
    "        continue\n",
    "    n_iix = bigram_fd_v2_s2[(w1, w2)]\n",
    "    n_ixi = wildcard_fd_v2_s2[(w1, w3)]\n",
    "    n_xii = bigram_fd_v2_s2[(w2, w3)]\n",
    "    n_ixx = word_fd_v2_s2[w1]\n",
    "    n_xix = word_fd_v2_s2[w2]\n",
    "    n_xxi = word_fd_v2_s2[w3]\n",
    "    score = pmi_v2(n_iii, (n_iix, n_ixi, n_xii), (n_ixx, n_xix, n_xxi), n_all)\n",
    "    \n",
    "    mi_score_dict_2[trigram[0]] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b99f9f-f0c3-4c18-a83c-2a781b0038f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list({k: v for k, v in sorted(mi_score_dict_2.items(), key=lambda item: item[1], reverse=True)})[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b59cc1-5a1c-4b59-af24-37d99ac27a6a",
   "metadata": {},
   "source": [
    "# Пункт 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6030cf-3c52-41c9-a6b7-3a3d1d04bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder_thr_1 = TrigramCollocationFinder(word_fd_v1_s, bigram_fd_v1_s, wildcard_fd_v1_s, ngram_fd_v1_s)\n",
    "print(finder_thr_1.nbest(trigram_measures.pmi, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c0a3c7-11f0-4e28-bc64-d91d0a055ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_nltk = finder_thr_1.score_ngrams(trigram_measures.pmi)\n",
    "for score_nltk in scores_nltk[:20]:\n",
    "    print(score_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4aa348-51c9-4536-b6b7-cbc86eb8dd62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03854f20-0725-4eb9-8038-4265676469c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "text = [item for sublist in sentences_test for item in sublist]\n",
    "finder_thr = TrigramCollocationFinder.from_words(text, 3)\n",
    "\n",
    "print(finder_thr.nbest(trigram_measures.pmi, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3726a2-e053-44a8-9884-13e378a37c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fd_v1, bigram_fd_v1, wildcard_fd_v1, ngram_fd_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda1e628-b40c-4b91-9577-8765eab699c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_thr.ngram_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0592989-d400-4e8d-a575-236d8cf8f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_thr.bigram_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc51df-ecee-4a83-ab73-c7f017536624",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_thr.wildcard_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c189e68-079a-44ea-b813-f16fdc2a48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_thr.word_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232c426-0ff1-495e-8c8e-a8bf3ab0257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ngrams = sum(ngram_counts.values())\n",
    "total_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aeaf30-6341-4009-bba9-bb3333c93baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
