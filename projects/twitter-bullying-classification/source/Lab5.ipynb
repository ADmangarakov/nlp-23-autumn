{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from chroma_db import ChromaDB\n",
    "from utils import split_into_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../assets/cyberbullying_tweets.csv\")\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentences\"] = df[\"tweet_text\"].apply(split_into_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoK2WyjEGSxr"
   },
   "source": [
    "## Var 1. BGE large model using HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-en-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-large-en-v1.5').to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_test\n",
    "concat_metadata = False\n",
    "batch_size = 512\n",
    "\n",
    "sentences_metadata = dict()\n",
    "\n",
    "cyberbullying_types = sorted(data[\"cyberbullying_type\"].unique().tolist())\n",
    "cyberbullying_tokens = tokenizer(cyberbullying_types, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    cyberbullying_emb = model(**cyberbullying_tokens)[0][:, 0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    sentence_embeddings = torch.empty((0, 1024), dtype=torch.float32)\n",
    "    cnt = 0\n",
    "    for cyberbullying_type in tqdm(cyberbullying_types):\n",
    "        subset = data[data[\"cyberbullying_type\"] == cyberbullying_type]\n",
    "        N = subset.shape[0]\n",
    "        iterations = math.ceil(N / batch_size)\n",
    "\n",
    "        for i in tqdm(range(iterations)):\n",
    "            sentences = list()\n",
    "            for index, text in subset[batch_size * i: batch_size * (i + 1)][\"sentences\"].items():\n",
    "                for j, sentence in enumerate(text):\n",
    "                    sentences.append(sentence)\n",
    "                    sentences_metadata[cnt] = {\n",
    "                        \"text_index\": index,\n",
    "                        \"sentence_index\": j,\n",
    "                        \"cyberbullying_type\": cyberbullying_type\n",
    "                    }\n",
    "                    cnt += 1\n",
    "\n",
    "            tokens = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "            model_output = model(**tokens)\n",
    "\n",
    "            embeddings = model_output[0][:, 0]\n",
    "\n",
    "            if concat_metadata:\n",
    "                embeddings += cyberbullying_emb[cyberbullying_types.index(cyberbullying_type)]\n",
    "\n",
    "            sentence_embeddings = torch.vstack((sentence_embeddings, embeddings.to(\"cpu\").detach().clone()))\n",
    "\n",
    "            model_output[0].to(\"cpu\")\n",
    "            model_output[1].to(\"cpu\")\n",
    "            del model_output\n",
    "            del tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "model.cpu()\n",
    "cyberbullying_tokens[0].to(\"cpu\")\n",
    "cyberbullying_tokens[1].to(\"cpu\")\n",
    "embeddings.to(\"cpu\")\n",
    "del model\n",
    "del cyberbullying_tokens\n",
    "del embeddings\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings_norm = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyiL-0rJ0DoI"
   },
   "source": [
    "## Var 2. BGE large model using Sentence-Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('BAAI/bge-large-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_test\n",
    "concat_metadata = False\n",
    "batch_size = 512\n",
    "\n",
    "sentences_metadata_v2 = dict()\n",
    "\n",
    "cyberbullying_types = sorted(data[\"cyberbullying_type\"].unique().tolist())\n",
    "\n",
    "with torch.no_grad():\n",
    "    sentence_embeddings_v2 = np.empty((0, 1024), dtype=float)\n",
    "    cnt = 0\n",
    "\n",
    "    N = data.shape[0]\n",
    "    iterations = math.ceil(N / batch_size)\n",
    "    for i in tqdm(range(iterations)):\n",
    "        sentences = list()\n",
    "        for index, row in data[batch_size * i: batch_size * (i + 1)][[\"sentences\", \"cyberbullying_type\"]].iterrows():\n",
    "            for j, sentence in enumerate(row[\"sentences\"]):\n",
    "                sentences.append(sentence)\n",
    "                sentences_metadata_v2[cnt] = {\n",
    "                    \"text_index\": index,\n",
    "                    \"sentence_index\": j,\n",
    "                    \"cyberbullying_type\": row[\"cyberbullying_type\"]\n",
    "                }\n",
    "                cnt += 1\n",
    "\n",
    "        sentence_embeddings_v2 = np.vstack((sentence_embeddings_v2, model.encode(sentences, normalize_embeddings=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzAYKh7jzvC6"
   },
   "source": [
    "## Создание ВБД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_ch = ChromaDB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_chroma_cosine = client_ch.get_collection(\"tweets_collection\", \"cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRse4svd9IgE"
   },
   "source": [
    "## Create and fill collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [sentence for text in data[\"sentences\"].tolist() for sentence in text]\n",
    "metadatas = [\n",
    "    {\n",
    "        \"cyberbullying_type\": v[\"cyberbullying_type\"],\n",
    "        \"text_index\": v[\"text_index\"],\n",
    "        \"sentence_index\": v[\"sentence_index\"]\n",
    "    } for _, v in sentences_metadata_v2.items()\n",
    "]\n",
    "\n",
    "ids = [f\"id{k}\" for k in sorted(sentences_metadata_v2.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_chroma_v2_cosine = client_ch.get_collection(\"tweets_collection_v2\", \"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_chroma_v2_cosine.add(sentence_embeddings_v2, texts, metadatas, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_chroma_v2_cosine.collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z9Xpcne7M-z"
   },
   "source": [
    "## Поиск схожих фрагментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyberbullying_types = ['age', 'ethnicity', 'gender', 'not_cyberbullying', 'other_cyberbullying', 'religion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поиск документов с упоминанием девушек в школах\n",
    "\n",
    "res1 = collection_chroma_v2_cosine.query(\n",
    "    query_texts=[\"Girl in school\"],\n",
    "    n_results=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1_2 = collection_chroma_v2_cosine.query(\n",
    "    query_embeddings=[model.encode(\"Girl in school\", normalize_embeddings=True).tolist()],\n",
    "    n_results=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1_2[\"ids\"][0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поиск документов с этническим кибербулингом афро\n",
    "\n",
    "res2 = collection_chroma_v2_cosine.query(\n",
    "    query_texts=[\"Black guy\"],\n",
    "    n_results=10,\n",
    "    where={\"cyberbullying_type\": \"ethnicity\"},\n",
    "    where_document={\"$contains\":\" black\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поиск документов с плохими словами\n",
    "\n",
    "res3 = collection_chroma_v2_cosine.query(\n",
    "    query_texts=[\"fuck\", \"hate\", \"dumb\"],\n",
    "    n_results=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Документы с плохими словами не использующие эти слова\n",
    "res4 = collection_chroma_v2_cosine.query(\n",
    "    query_texts=[\"fuck\", \"hate\", \"dumb\"],\n",
    "    n_results=5,\n",
    "    where_document={\n",
    "        \"$and\": [\n",
    "            {\n",
    "                \"$not_contains\": \"FUCK,fuck,Fuck\",\n",
    "            },\n",
    "            {\n",
    "                \"$not_contains\": \"hate\",\n",
    "            },\n",
    "            {\n",
    "                \"$not_contains\": \"dumb, DUMB, Dumb\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_array = [\n",
    "    ((17920, 0), \"What the OLF Qarro group is doing?\"),\n",
    "    ((19137, 0), \"Who does Blumenthal love?\"),\n",
    "    ((5755, 0), \"Who has several chlorine production plants?\"),\n",
    "    ((17311, 2), \"Who strongly believes homosexuality is a sin?\"),\n",
    "    ((19035, 0), \"Which country sells their sons to terrorists?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df_test[df_test.index == 19035]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test.index == 17920]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Represent this sentence for searching relevant passages:\"\n",
    "collection = collection_chroma_v2_cosine\n",
    "search_results = []\n",
    "for q in questions_array:\n",
    "    query = instruction + \" \" + q[1]\n",
    "    query = model.encode(query, normalize_embeddings=True)\n",
    "    results = collection.query(50, query_embeddings=[query.tolist()])\n",
    "    id = list(dict(filter(lambda x: x[1][\"text_index\"] == q[0][0] and x[1][\"sentence_index\"] == q[0][1], sentences_metadata_v2.items())).keys())[0]\n",
    "    search_results.append(results[\"ids\"][0].index(f\"id{id}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "mean(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
