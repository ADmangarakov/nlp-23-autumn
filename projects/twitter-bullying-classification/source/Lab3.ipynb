{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766954ec-7ef4-4cd3-a1f4-16da2220e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex\n",
    "import math\n",
    "import csv\n",
    "from collections import Counter, OrderedDict\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import spatial\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11cff2-441e-49a8-9698-ffa10e8d6e7a",
   "metadata": {},
   "source": [
    "# Пункт 1. Чтение данных и построение матрицы Term-Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb9ed8-c2bf-485a-9407-3fa3ff961895",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "emotiocons = r'(?:(?::|;|=)(?:-|_)?(?:\\)|\\(|D|P))|(?:[-*]_[-*])'\n",
    "ending_signs = r'(?:\\?|\\.|\\.\\.\\.|\\!|\\?\\!|\\!\\?)$'\n",
    "\n",
    "def read_data(subset=\"train\", squeeze_text=True, collect_analysis=True):\n",
    "\n",
    "    # if squeeze_text=True, then we won't split text by sentences and each text will consist with one array of tokens\n",
    "    # if squeeze_text=False, then wil be created corpus of all sentences\n",
    "    \n",
    "    token_frequency = Counter()\n",
    "    term_document_matrix = Counter()\n",
    "    term_document_matrix_v2 = dict()\n",
    "    \n",
    "    texts = dict()\n",
    "    \n",
    "    # Перебираем папки\n",
    "    for folder in tqdm(['age', 'ethnicity', 'gender', 'religion', 'other_cyberbullying', 'not_cyberbullying']):\n",
    "        # Путь к папке\n",
    "        folder_path = os.path.join(f'../assets/annotated-corpus/', subset, folder)\n",
    "        # Перебираем файлы в папке\n",
    "        for file in tqdm(os.listdir(folder_path)):\n",
    "            # Если это tsv файл\n",
    "            if file.endswith('.tsv'):\n",
    "                # Путь к файлу\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                # Читаем файл\n",
    "                try: \n",
    "                    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    continue\n",
    "                    \n",
    "                # Группируем токены по предложениям (предполагает, что предложение отделено пустой строкой)\n",
    "                text = list()\n",
    "                sentence = list()\n",
    "                tokens_list = df[0].tolist()\n",
    "                for token in tokens_list:\n",
    "                    token = str(token).lower()\n",
    "                    if (regex.search(emotiocons, token) is None and regex.search(ending_signs, token) is not None) \\\n",
    "                        or token in stop_words:\n",
    "                        continue\n",
    "                    \n",
    "                    if squeeze_text:\n",
    "                        if token == 'nan':\n",
    "                            continue\n",
    "                        text.append(token)\n",
    "                    else:\n",
    "                        if token == 'nan':\n",
    "                            if len(sentence) > 0:\n",
    "                                text.append(sentence)\n",
    "                            sentence = []\n",
    "                            continue\n",
    "                        else:\n",
    "                            sentence.append(token)\n",
    "                    \n",
    "                    if collect_analysis:\n",
    "                        token_frequency[token] += 1\n",
    "                        doc_name = f\"{folder}_{file.rsplit('.', 1)[0]}\"\n",
    "                        term_document_matrix[(token, doc_name)] += 1\n",
    "                        if token not in term_document_matrix_v2:\n",
    "                            term_document_matrix_v2[token] = { doc_name: 1 }\n",
    "                        else:\n",
    "                            if doc_name not in term_document_matrix_v2[token]:\n",
    "                                term_document_matrix_v2[token][doc_name] = 1\n",
    "                            else:\n",
    "                                term_document_matrix_v2[token][doc_name] += 1\n",
    "                \n",
    "                if not squeeze_text and len(sentence) > 0:\n",
    "                    text.append(sentence)\n",
    "                \n",
    "                if len(text) > 0:\n",
    "                    texts[f\"{folder}_{file.rsplit('.', 1)[0]}\"] = text\n",
    "                    \n",
    "    return texts, token_frequency, term_document_matrix, term_document_matrix_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e5c0c-591f-477b-9d64-6dfea5024cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, token_frequency, term_document_matrix, term_document_matrix_v2 = read_data(\"train\", True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab908b-cf53-476e-95d9-2fdc367f48c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts, _, _, _ = read_data(\"test\", True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440432b-4b23-4927-a06a-1b890f957bf3",
   "metadata": {},
   "source": [
    "# Пункт 2. Построение матрицы TD-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d640e-5205-4e36-b703-dda77f721894",
   "metadata": {},
   "source": [
    "### Чистим данные от редких токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc22a6d-1c03-4c7d-8f04-7e45b7ca107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_frequency_copy = token_frequency.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590cdaff-9e79-4974-9445-17e76b25d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in token_frequency.items():\n",
    "    if v < 2:\n",
    "        del token_frequency_copy[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2554bad-9e49-445d-ab49-0ffd983aff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Находим токены, которые встречаются 1 раз\n",
    "rare_tokens = dict(filter(lambda x: x[1] < 2, token_frequency.items())).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a0ef0-4909-4d71-a8e9-35f5f81d7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_frequency), len(token_frequency) - len(rare_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d430cdf3-17e9-4958-aee8-5a6fffaeb5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dceb28-3275-45bf-8e57-37175571c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_document_matrix_copy_v2 = term_document_matrix_v2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1138f-e91c-4112-b84d-1f4d0d2a4e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in term_document_matrix_v2.items():\n",
    "    if k in rare_tokens:\n",
    "        del term_document_matrix_copy_v2[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468029b6-bc35-4e3c-8eea-74882561c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(term_document_matrix_v2), len(term_document_matrix_copy_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692085f4-f55d-4653-8df6-2c611f46254b",
   "metadata": {},
   "source": [
    "### Строим матрицу Document-Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d63911-9203-425a-b55e-284a455558fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_matrix_v2  = dict()\n",
    "\n",
    "for term, v in term_document_matrix_copy_v2.items():\n",
    "    for doc, val in v.items():\n",
    "        if doc not in document_term_matrix_v2:\n",
    "            document_term_matrix_v2[doc] = { term: val }\n",
    "        else:\n",
    "            document_term_matrix_v2[doc][term] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ef1be-57b8-425e-93bb-aaf10e2f3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict(filter(lambda x: len(x[1]) > 1, document_term_matrix_v2.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c0370-89db-4b37-9859-4e1158de98ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict(filter(lambda x: len(x[1]) > 1, term_document_matrix_v2.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38973f3-5ecf-4d43-8b90-8cc3e1fa2574",
   "metadata": {},
   "source": [
    "### Строим матрицу TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eacab7c-a51b-4d61-883a-8d881b9fef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix = term_document_matrix_copy_v2\n",
    "dt_matrix = document_term_matrix_v2\n",
    "\n",
    "documents = dt_matrix.keys()\n",
    "n = len(dt_matrix)\n",
    "\n",
    "tf_idf = dict()\n",
    "\n",
    "for doc, term_vals in dt_matrix.items():\n",
    "    temp_calc = dict()\n",
    "    for term, val in term_vals.items():\n",
    "        tf = val / sum(term_vals.values())\n",
    "        idf = math.log(n / len(td_matrix[term].keys()))\n",
    "        temp_calc[term] = tf * idf\n",
    "    tf_idf[doc] = temp_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cad0a6-dcaa-4e82-a8ff-c162f1d51ab7",
   "metadata": {},
   "source": [
    "#  Пункт 3\n",
    "Реализовать метод, позволяющий векторизовать произвольный текст с использованием нейронных сетей (предлагается использовать стандартную реализацию модели w2v или glove). Выбранную модель необходимо обучить на обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e353b433-8405-4456-b8a6-f5290156d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_texts = list(train_texts.values())\n",
    "model = Word2Vec(sentences=pure_texts, vector_size=100, window=3, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e3e7ec-c279-4f9d-a822-7f53403d32ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(pure_texts, total_examples=len(pure_texts), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532441c7-517a-4dc1-ac6b-46f6c895393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9769aed-fa27-42ae-adcf-11c0d14ff593",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = model.wv['black']\n",
    "vector2 = model.wv['white']\n",
    "\n",
    "vector3 = model.wv['fuck']\n",
    "vector4 = model.wv['dumb']\n",
    "vector5 = model.wv['bitch']\n",
    "vector6 = model.wv['idiot']\n",
    "vector7 = model.wv['stupid']\n",
    "\n",
    "vector8 = model.wv['people']\n",
    "vector9 = model.wv['girl']\n",
    "vector10 = model.wv['woman']\n",
    "vector11 = model.wv['man']\n",
    "\n",
    "vector12 = model.wv['islam']\n",
    "vector13 = model.wv['muslim']\n",
    "vector14 = model.wv['christian']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ad7a0-8590-4e2f-9d95-e053c582b4d8",
   "metadata": {},
   "source": [
    "# Пункт 4\n",
    "\n",
    "Рассмотрим насколько близки между собой токены, выбранные в 3 пункте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6da6f9-4e5b-4bee-80d1-fd5cb018971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_lib(vec1, vec2):\n",
    "    return 1 - spatial.distance.cosine(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a290e-7c1d-4ad0-9476-d70112ca930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vec1, vec2):\n",
    "    assert len(vec1) == len(vec2) and not isinstance(vec1[0], list)\n",
    "    dot12, norm1, norm2 = 0, 0, 0\n",
    "    for x1, x2 in zip(vec1, vec2):\n",
    "        dot12 += x1 * x2\n",
    "        norm1 += x1 * x1\n",
    "        norm2 += x2 * x2\n",
    "    return dot12 / math.sqrt(norm1 * norm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f91bf-fb54-4e14-ae95-ad23a32ce08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim(vector1, vector2), cosine_sim_lib(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8553f-5fed-450b-b67f-52494a5b174c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9317a820-aadb-4698-90db-c3a1d3ce8020",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.key_to_index)\n",
    "X = [model.wv[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84674c5b-b930-42e7-9b20-ec5206ab1ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ef0af-1307-4082-bfbd-4173a348bc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=500)\n",
    "\n",
    "for vec, name in zip([vector1, vector2],['black', 'white']):\n",
    "    reduced_vec = pca.transform(vec[np.newaxis, ...])\n",
    "    plt.plot(reduced_vec[0][0], reduced_vec[0][1], 'o', color='red')\n",
    "    plt.annotate(name, (reduced_vec[0][0], reduced_vec[0][1]))\n",
    "\n",
    "for vec, name in zip([vector3, vector4, vector5, vector6, vector7], ['fuck', 'dumb', 'bitch', 'idiot', 'stupid']):\n",
    "    reduced_vec = pca.transform(vec[np.newaxis, ...])\n",
    "    plt.plot(reduced_vec[0][0], reduced_vec[0][1], 'o', color='blue')\n",
    "    plt.annotate(name, (reduced_vec[0][0], reduced_vec[0][1]))\n",
    "    \n",
    "for vec, name in zip([vector8, vector9, vector10, vector11], ['people', 'girl', 'woman', 'man']):\n",
    "    reduced_vec = pca.transform(vec[np.newaxis, ...])\n",
    "    plt.plot(reduced_vec[0][0], reduced_vec[0][1], 'o', color='orange')\n",
    "    plt.annotate(name, (reduced_vec[0][0], reduced_vec[0][1]))\n",
    "    \n",
    "for vec, name in zip([vector12, vector13, vector14], ['islam', 'muslim', 'christian']):\n",
    "    reduced_vec = pca.transform(vec[np.newaxis, ...])\n",
    "    plt.plot(reduced_vec[0][0], reduced_vec[0][1], 'o', color='green')\n",
    "    plt.annotate(name, (reduced_vec[0][0], reduced_vec[0][1]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d28848-5fca-46ab-8531-6d720ae193e4",
   "metadata": {},
   "source": [
    "# Пункт 5\n",
    "Сократим размерность матрицы TF-IDF. Для этого преобразуем её из эффективного формата хранения в разреженную матрицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a73c5-4145-4d56-bc79-3dfcf31ef1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(td_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ee436-07a3-47e3-8545-c3e0fc15ddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = list(td_matrix)\n",
    "docs = list(dt_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68577b0-c7ca-4725-8278-5b0bb458dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_data = np.full((len(docs), len(terms)), 0, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b874ad-7263-4a15-9c95-209c8a15107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in tqdm(enumerate(docs)):\n",
    "    for j, term in enumerate(terms):\n",
    "        if term in tf_idf[doc]:\n",
    "            sparse_data[i, j] = tf_idf[doc][term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45ee7c-928a-4a6b-86a4-0806c03f0cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c60026b-75b0-41bf-a0d5-47c36276cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_v2 = PCA(n_components=100)\n",
    "sparse_pca_data = pca_v2.fit_transform(sparse_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aae602-ba61-498a-866c-1a33c85d8d51",
   "metadata": {},
   "source": [
    "# Пункт 6\n",
    "С использованием разработанного метода подсчета косинусного расстояния сравнить эффективность метода векторизации с использованием нейронных сетей и эффективность базовых методов векторизации с последующим сокращением размерности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224678d-c854-4009-a001-e9a5aa058209",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf[docs[500]].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7da59f-2924-4c35-bda3-7f3fd780aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = np.full((len(docs), 100), 0, dtype=np.float32)\n",
    "for i, doc in enumerate(docs):\n",
    "    temp = np.full(100, 0, dtype=np.float32)\n",
    "    cnt = 0\n",
    "    for word in tf_idf[doc].keys():\n",
    "        temp += model.wv[word]\n",
    "        cnt += 1 \n",
    "    model_data[i] = temp / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b0a8a-a2d2-4e15-9914-cf6f00005e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_v1 = dict(sorted(token_frequency.items(), key=lambda x: x[1], reverse=True)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b7ba6-d78d-4389-8969-98f1ba16c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_v2 = { k: v for k, v in token_frequency.items() if v > 200 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d62155e-f742-4016-b21c-95b308459082",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_words_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10598dd3-6877-42e3-bd54-6770bbc09724",
   "metadata": {},
   "source": [
    "Подберём тексты, содержащие популярные слова, при этом с разной меткой и минимальным числом слов 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d953a5c-1c0e-4bbf-b88e-d2caa1a4b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_doc_valid(doc_name, words, category, num_words, must_words):\n",
    "    return category in doc_name and len(words) >= num_words and len(words.intersection(must_words)) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc3549a-ff07-497f-b8ce-aec56b501130",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_subset = list()\n",
    "texts_cnt = {\n",
    "    'age': 0, 'ethnicity': 0, 'gender': 0, 'religion': 0, 'other_cyberbullying': 0, 'not_cyberbullying': 0\n",
    "}\n",
    "num_words = 5\n",
    "num_docs_per_category = 3\n",
    "\n",
    "must_words = set(top_words_v1)\n",
    "for doc, words in dt_matrix.items():\n",
    "    ws = set(words.keys())\n",
    "    for category in texts_cnt.keys():\n",
    "        if texts_cnt[category] < num_docs_per_category and is_doc_valid(doc, ws, category, num_words, must_words):\n",
    "            texts_cnt[category] += 1\n",
    "            docs_subset.append(doc)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c43f448-ba53-48ea-99e7-b93ce16d2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef96ae2-73a7-46b4-ad11-186ea02ba010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef064901-f608-4a36-9742-1a29d9764809",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_doc = docs_subset[0]\n",
    "base_index = docs.index(base_doc)\n",
    "for doc in docs_subset:\n",
    "    index = docs.index(doc)\n",
    "    print(f\"Words2Vec: {base_doc} <-> {doc}:\", cosine_sim(model_data[base_index], model_data[index]))\n",
    "    print(f\"TF-IDF: {base_doc} <-> {doc}:\", cosine_sim(sparse_data[base_index], sparse_data[index]))\n",
    "    print(f\"TF-IDF with PCA: {base_doc} <-> {doc}:\", cosine_sim(sparse_pca_data[base_index], sparse_pca_data[index]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67052ea1-0f9f-4a0a-9a34-467367661af5",
   "metadata": {},
   "source": [
    "# Пункт 7\n",
    "Реализовать метод, осуществляющий векторизацию произвольного текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742c593-485c-4717-8fc3-3b0e5ae2e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_td_matrix(td_matrix):\n",
    "    dt_matrix  = dict()\n",
    "\n",
    "    for term, v in td_matrix.items():\n",
    "        for doc, val in v.items():\n",
    "            if doc not in dt_matrix:\n",
    "                dt_matrix[doc] = { term: val }\n",
    "            else:\n",
    "                dt_matrix[doc][term] = val\n",
    "    return dt_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cebbf7-bf2f-4c32-82a5-f1a780e7ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tf_idf(td_matrix, dt_matrix):\n",
    "    n = len(dt_matrix)  \n",
    "    tf_idf = dict()\n",
    "    \n",
    "    for doc, term_vals in tqdm(dt_matrix.items()):\n",
    "        temp_calc = dict()\n",
    "        for term, val in term_vals.items():\n",
    "            tf = val / sum(term_vals.values())\n",
    "            idf = math.log(n / len(td_matrix[term].keys()))\n",
    "            temp_calc[term] = tf * idf\n",
    "        tf_idf[doc] = temp_calc\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b99911-45f6-41a1-8e92-94d8be232d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_texts(texts, rare_tokens):\n",
    "    texts_copy = OrderedDict()\n",
    "    for key, text in tqdm(texts.items()):\n",
    "        text_copy = list()\n",
    "        for sentence in text:\n",
    "            sentence_copy = list()\n",
    "            for word in sentence:\n",
    "                if word not in rare_tokens:\n",
    "                    sentence_copy.append(word)\n",
    "            if len(sentence_copy) > 0:\n",
    "                text_copy.append(sentence_copy)\n",
    "        if len(text_copy) > 0:\n",
    "            texts_copy[key] = text_copy\n",
    "    return texts_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997aa0d-9735-47a9-8530-a80aac790a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_texts(subset, clear_rare_tokens=True):\n",
    "    print(\"Reading data and calculating term-document matrices\")\n",
    "    texts, token_frequency_base, term_document_matrix, td_matrix_base = read_data(subset, False, True)\n",
    "\n",
    "    texts = OrderedDict(texts)\n",
    "\n",
    "    if clear_rare_tokens:\n",
    "        print(\"Delete rare tokens in token_frequency\")\n",
    "        rare_tokens = dict(filter(lambda x: x[1] < 2, token_frequency.items())).keys()\n",
    "\n",
    "        print(\"Delete rare tokens in term-document matrix\")\n",
    "        td_matrix = td_matrix_base.copy()\n",
    "        for k, v in td_matrix_base.items():\n",
    "            if k in rare_tokens:\n",
    "                del td_matrix[k]\n",
    "\n",
    "        print(\"Clear texts\")\n",
    "        texts = clear_texts(texts, rare_tokens)\n",
    "\n",
    "    else:\n",
    "        td_matrix = td_matrix_base\n",
    "\n",
    "    print(\"Build document-term matrix\")\n",
    "    dt_matrix = inverse_td_matrix(td_matrix)\n",
    "\n",
    "    print(\"Calculate TF-IDF\")\n",
    "    tf_idf = calc_tf_idf(td_matrix, dt_matrix)\n",
    "\n",
    "    pure_texts = list(map(lambda x: reduce(add, x), texts.values()))\n",
    "\n",
    "    print(\"Train Word2Vec\")\n",
    "    model = Word2Vec(sentences=pure_texts, vector_size=100, window=3, min_count=1, workers=4)\n",
    "    model.train(pure_texts, total_examples=len(pure_texts), epochs=100)\n",
    "\n",
    "    print(\"Vectorize texts\")\n",
    "    model_data = np.full((len(dt_matrix), 100), 0, dtype=np.float32)\n",
    "    for i, (doc, sentences) in tqdm(enumerate(texts.items())):\n",
    "        temp = np.empty((0, 100), dtype=np.float32)\n",
    "        cnt = 0\n",
    "        for sentence in sentences:\n",
    "            sentence_vec = np.full(100, 0, dtype=np.float32)\n",
    "            for word in sentence:\n",
    "                sentence_vec += model.wv[word] * tf_idf[doc][word]\n",
    "            temp = np.vstack((temp, sentence_vec / sum(tf_idf[doc].values())))\n",
    "\n",
    "        model_data[i] = np.mean(temp, axis=0)\n",
    "\n",
    "    return texts, model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a2eae-14cf-433c-813a-1096e23941cb",
   "metadata": {},
   "source": [
    "# Пункт 8\n",
    "Выполнить векторизацию тестовой выборки с использованием метода, реализованного на предыдущем шаге. Результаты сохранить в формате tsv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd0879-c165-4e0d-b62a-001ae17f18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, test_vecs = vectorize_texts(\"test\", clear_rare_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee53f2de-bb81-4576-82a8-46e41a9a2ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e16f791-b8b0-4453-878c-ccd3ced8bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_v2, train_vecs = vectorize_texts(\"train\", clear_rare_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fd9cd-fe4e-41ea-adb2-7c1aca331e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f911981-a0d3-49a4-93ba-27b4b483864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(texts, vectors, subset=\"test\"):\n",
    "    with open(f'./assets/annotated-corpus/{subset}-embeddings.tsv', 'w', newline='') as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n",
    "        for i, doc_name in enumerate(texts.keys()):\n",
    "            writer.writerow([doc_name] + vectors[i].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a6a69-86da-45a3-8c57-94eeaaed61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(texts, test_vecs, subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d86d763-b0ae-4b2b-befa-0ab67b7bfab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(train_texts_v2, train_vecs, subset=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
