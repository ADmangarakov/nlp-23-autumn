{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model_emb = SentenceTransformer('BAAI/bge-large-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"./../assets/test.csv\",header=None)\n",
    "data = data.values\n",
    "for i in range(len(data)):\n",
    "    data[i][0]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs =  []\n",
    "from tqdm import tqdm\n",
    "for el in tqdm(data):\n",
    "    embs = model_emb.encode([el[2]], normalize_embeddings=True)\n",
    "    all_embs.append((el[0],embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.Client()\n",
    "\n",
    "collection = client.create_collection(\"sample_collection7\")\n",
    "abc = [el[1][0].tolist() for el in all_embs]\n",
    "collection.add(\n",
    "    embeddings=abc,\n",
    "    metadatas=[{\"headline\":el} for el in data[:,1].tolist()],\n",
    "    ids=[str(el[0]) for el in all_embs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(q):\n",
    "    instruction = \"Represent this sentence for searching relevant passages:\"\n",
    "    query = instruction + \" \" + q\n",
    "    query = model_emb.encode(query, normalize_embeddings=True)\n",
    "    results = collection.query(query_embeddings=[query.tolist()],n_results=10)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(question):\n",
    "    docs = [data[int(el)][2] for el in get_documents(question)[\"ids\"][0]]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")  \n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\").half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Why did IBM acquired some Danish IT firms?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = f\"Answer the question using the provided context. Your answer should be in your own words and be no longer than 30 words. You should ignore documents that do not contain relevant information. if you can't answer the question, say so. Return only the Answer to the Question, if you answered the question, end the sentence, do not ask further questions. \\n\\n Context: {context} \\n\\n Question: {query} \\n\\n Answer:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_msg(query, history):\n",
    "    docs = get_answer(query)\n",
    "    context = \"\"\n",
    "    for i in range(len(docs)):\n",
    "        context+=f\"{i+1}. {docs[i]}  \"\n",
    "    prompt = f\"Answer the question using the provided context. Your answer should be in your own words and be no longer than 30 words. You should ignore documents that do not contain relevant information. if you can't answer the question, say so. Return only the Answer to the Question, if you answered the question, end the sentence, DO NOT ASK FURTHER QUESTIONS!. \\n\\n Context: {context} \\n\\n Question: {query} \\n\\n Answer:\"\n",
    "    with torch.inference_mode():\n",
    "        qqq = tokenizer(prompt, return_tensors='pt').input_ids.cuda()\n",
    "        a = model.generate(qqq,max_new_tokens=100)\n",
    "        b = tokenizer.decode(a[0][len(qqq[0]):])\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_answer(\"What is the Pride of Aloha?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_msg(\"Why there is a conflict of interest existing between Chelsea and CSKA Moscow?\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# def echo(message, history):\n",
    "#     return message\n",
    "\n",
    "demo = gr.ChatInterface(fn=chat_msg, examples=[\"hello\", \"hola\", \"merhaba\"], title=\"Echo Bot\")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
