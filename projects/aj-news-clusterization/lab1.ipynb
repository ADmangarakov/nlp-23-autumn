{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e417be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13cdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_1.csv', header=None, names = ['topic','title', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a4a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    email_regex = r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\"\n",
    "    phone_regex = \"\\+?[1-9]\\-?\\(?[0-9]{3}\\)?\\-?[0-9]{2,4}\\-?[0-9]{2,4}\"\n",
    "    link_regex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "    word_regex = r'\\w+'\n",
    "\n",
    "    combined_regex = \"|\".join((email_regex, phone_regex, link_regex, word_regex))\n",
    "\n",
    "    tokens = re.findall(combined_regex, text)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b268a28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenization(\"This fvfv +47854434555  had221@gmd.com callicarpa epimedium churchyard hangs; over the law the shadow. His callicarpa, part, may - not a; bit that all the. Anything - penstemon, had - forgotten you - may have now. Her, stokesia as if we meet van helsing i can have? Reading epimedium campanula; in time his stately; gravity; i presume, that you. Prevalent; campanula by pirates off on. I dined too prejudiced https://bit.ly/tfjqpibm you - oh, mina write))))))) Phrase betula berberis - callicarpa callicarpa; project gutenberg tm electronic works 1 e never. Panes stokesia berberis callicarpa there is to see that. And - rudbeckia https://bit.ly/oaghfwcj penstemon, monarda three of one - might frighten. ,flowers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a866ad39",
   "metadata": {},
   "source": [
    "### Стэмминг и лемматизация nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d876ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stemmer_en = SnowballStemmer('english')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def convert_pos_wordnet(pos):\n",
    "    if pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def stemming(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    stemmed_words = [stemmer_en.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "def lemmatization(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    wordnet_tags = [convert_pos_wordnet(tag[1]) for tag in tagged_words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(tag[0], tag[1]) for tag in zip(words, wordnet_tags)]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "def split_into_sentences(text: str):\n",
    "\n",
    "\n",
    "    text = text.replace(\"...\",\"<three_points><stop>\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"??\",\"<two_q><stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<three_points>\",\"...\")\n",
    "    text = text.replace(\"<two_q>\",\"??\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences[:-1]\n",
    "\n",
    "def process_text(text):\n",
    "    sentences = split_into_sentences(text)\n",
    "\n",
    "    tokens_stems_lemmas = []\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenization(sentence)\n",
    "        stems = [stemmer_en.stem(token) for token in tokens]\n",
    "        lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokens_stems_lemmas.append(list(zip(tokens, stems, lemmas)))\n",
    "\n",
    "    return tokens_stems_lemmas\n",
    "\n",
    "df['processed_text'] = df['text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684eb6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"bisfg fgfdg. sfg fgweon? fgdsf uijuijui hbh. iuiniuni jni ! YUUHiininifd sindf hinhisdf... dg iihifhihifhi.\"\n",
    "split_into_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49d78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotation(df):\n",
    "    for i in df['topic'].unique():\n",
    "        path = 'assets/annotated_corpus/test/'+str(i)\n",
    "        isExist = os.path.exists(path)\n",
    "        if not isExist:\n",
    "            os.makedirs(path)\n",
    "        \n",
    "    for index, row in df.iterrows():\n",
    "        output_filename = f\"assets/annotated_corpus/test/{str(row['topic'])}/annotation_{index}.tsv\"\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            for sentence in row['processed_text']:\n",
    "                for token, stem, lemma in sentence:\n",
    "                    f.write(f\"{token}\\t{stem}\\t{lemma}\\n\")\n",
    "                f.write(\"\\n\")  \n",
    "\n",
    "\n",
    "create_annotation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dee73b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
