{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d8e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_lemmas = []\n",
    "\n",
    "for folder in tqdm(['1', '2', '3', '4']):\n",
    "    folder_path = os.path.join('assets/annotated_corpus/train/', folder)\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        try:\n",
    "\n",
    "            if file.endswith('.tsv') and file.startswith('annotation'):\n",
    "\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "\n",
    "                df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "\n",
    "                lemma_list = df[0].tolist()\n",
    "                sentence_lemmas = []\n",
    "                for lemma in lemma_list:\n",
    "                    if str(lemma) != 'nan':\n",
    "                        sentence_lemmas.append(lemma)\n",
    "                    else:\n",
    "                        all_lemmas.append(sentence_lemmas)\n",
    "                        sentence_lemmas = []\n",
    "\n",
    "                if len(sentence_lemmas) > 0:\n",
    "                    all_lemmas.append(sentence_lemmas)\n",
    "        except: Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3a0683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "all_lemmas_test = []\n",
    "\n",
    "for folder in tqdm(['1', '2', '3', '4']):\n",
    "    folder_path = os.path.join('assets/annotated_corpus/test/', folder)\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "\n",
    "        if file.endswith('.tsv') and file.startswith('annotation'):\n",
    "\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "\n",
    "            df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "\n",
    "            lemma_list = df[0].tolist()\n",
    "            sentence_lemmas = []\n",
    "            for lemma in lemma_list:\n",
    "                if str(lemma) != 'nan':\n",
    "                    sentence_lemmas.append(lemma)\n",
    "                else:\n",
    "                    all_lemmas_test.append(sentence_lemmas)\n",
    "                    sentence_lemmas = []\n",
    "\n",
    "            if len(sentence_lemmas) > 0:\n",
    "                all_lemmas_test.append(sentence_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1834444",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lemmas = all_lemmas_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace8697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [item for sublist in all_lemmas for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d8e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea15c36",
   "metadata": {},
   "source": [
    "#### Delete stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b2a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered = []\n",
    " \n",
    "for w in all_words:\n",
    "    if w not in stop_words:\n",
    "        filtered.append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7892e3",
   "metadata": {},
   "source": [
    "#### count words and delete rear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46da3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for word in filtered:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3233444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rear_words = [k for k, v in counts.items() if v < 5]\n",
    "\n",
    "filtered_final = [filt for filt in tqdm(filtered) if filt not in rear_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_filtered = {}\n",
    "for word in filtered_final:\n",
    "        if word in counted_filtered:\n",
    "            counted_filtered[word] += 1\n",
    "        else:\n",
    "            counted_filtered[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1860c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: v for k, v in sorted(counted_filtered.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6292c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"counted_tokens.json\", \"w\") as file:\n",
    "    json.dump({k: v for k, v in sorted(counted_filtered.items(), key=lambda item: item[1], reverse=True)}, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0283777",
   "metadata": {},
   "source": [
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0cf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_term_idf(term_document_matrix, terms):\n",
    "    tdm_np = np.array(term_document_matrix)\n",
    "    term_idf = {}\n",
    "    \n",
    "    for term, i in zip(terms, range(len(terms))):\n",
    "        term_idf[term] = np.count_nonzero(tdm_np[:,i])\n",
    "        \n",
    "    return term_idf\n",
    "\n",
    "\n",
    "def create_term_document_matrix(docs, counted_filtered):\n",
    "    term_document_matrix = []\n",
    "    terms = list(counted_filtered.keys())\n",
    "    for doc in tqdm(docs):\n",
    "        row = [doc.count(term) for term in terms]\n",
    "        term_document_matrix.append(row)\n",
    "\n",
    "    return term_document_matrix, terms\n",
    "\n",
    "N = len(all_lemmas)\n",
    "\n",
    "def compute_tf_idf(term_document_matrix, term_doc_count, terms):\n",
    "    tf_idf_matrix = []\n",
    "    for row in tqdm(term_document_matrix):\n",
    "        tf_idf_row = [(tf * log(N / term_idf[term])) for tf, term in zip(row, terms)]\n",
    "        tf_idf_matrix.append(tf_idf_row)\n",
    "\n",
    "    return tf_idf_matrix\n",
    "\n",
    "def get_doc_vector(matrix, sent):\n",
    "    return matrix[sent]\n",
    "\n",
    "def get_word_vector(matrix, word):\n",
    "    return np.array(matrix)[:, word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea58a3",
   "metadata": {},
   "source": [
    "**Считаем матрицу term doc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081af31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_document_matrix, terms = create_term_document_matrix(all_lemmas, counted_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e55e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"term_document_matrix_tr\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(term_document_matrix, fp)\n",
    "with open(\"terms_tr\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(terms, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=500)\n",
    "plt.imshow(np.asarray(term_document_matrix), interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776c662",
   "metadata": {},
   "source": [
    "Считаем idf для каждого слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_idf = compute_term_idf(term_document_matrix, terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85ad4bd",
   "metadata": {},
   "source": [
    "#### Считаем tf-idf матрицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb627a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix = compute_tf_idf(term_document_matrix, term_idf, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd91856",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=500)\n",
    "plt.imshow(np.asarray(tf_idf_matrix), interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c98cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_vector(tf_idf_matrix,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3778133",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_doc_vector(tf_idf_matrix,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = get_word_vector(tf_idf_matrix,terms.index('monday'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a326f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector2 = get_word_vector(tf_idf_matrix,terms.index('wednesday'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbde1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_similarity(vector1, vector2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9852776",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "# model = Word2Vec(sentences=all_lemmas, vector_size=100, window=3, min_count=1, workers=4)\n",
    "# model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c1e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88599611",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(all_lemmas, total_examples=len(all_lemmas), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96291d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = model.wv['georgian']\n",
    "vector2 = model.wv['british']\n",
    "\n",
    "vector3 = model.wv['war']\n",
    "vector4 = model.wv['crime']\n",
    "vector4_1 = model.wv['soldier']\n",
    "vector4_2 = model.wv['military']\n",
    "\n",
    "\n",
    "\n",
    "vector5 = model.wv['republican']\n",
    "vector6 = model.wv['democrat']\n",
    "\n",
    "\n",
    "vector7 = model.wv['tuesday']\n",
    "vector8 = model.wv['monday']\n",
    "vector9 = model.wv['sunday']\n",
    "vector10 = model.wv['wednesday']\n",
    "\n",
    "vector11 = model.wv['afghanistan']\n",
    "vector12 = model.wv['iraq']\n",
    "vector12_1 = model.wv['iran']\n",
    "\n",
    "\n",
    "\n",
    "vector13 = model.wv['panda']\n",
    "vector14 = model.wv['monkey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90abf545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "1 - distance.cosine(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b458a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cosine_similarity(v1,v2):\n",
    "    \"compute cosine similarity of v1 to v2: (v1 dot v2)/{||v1||*||v2||)\"\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]\n",
    "        y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy/math.sqrt(sumxx*sumyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff441ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_similarity(vector1, vector2))\n",
    "print(cosine_similarity(vector3, vector4))\n",
    "print(cosine_similarity(vector7, vector8))\n",
    "print(cosine_similarity(vector11, vector12))\n",
    "print(cosine_similarity(vector13, vector14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2250369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "words = list(model.wv.key_to_index)\n",
    "X = [model.wv[word] for i, word in enumerate(words)]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54655e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=500)\n",
    "\n",
    "for vec, name in zip([vector1,vector2],['georgian', 'british']):\n",
    "    plt.plot(pca.transform(vec.reshape(1, -1))[0][0], pca.transform(vec.reshape(1, -1))[0][1], 'o', color='red')\n",
    "    plt.annotate(name, (pca.transform(vec.reshape(1, -1))[0][0], pca.transform(vec.reshape(1, -1))[0][1]))\n",
    "\n",
    "for vec, name in zip([vector3,vector4,vector4_1,vector4_2], ['war','crime','soldier','military']):\n",
    "    plt.plot(pca.transform(vec.reshape(1, -1))[0][0], pca.transform(vec.reshape(1, -1))[0][1], 'o', color='blue')\n",
    "    plt.annotate(name, (pca.transform(vec.reshape(1, -1))[0][0], pca.transform(vec.reshape(1, -1))[0][1]))\n",
    "    \n",
    "for vec, name in zip([vector5,vector6], ['republican', 'democrat']):\n",
    "    plt.plot(pca.transform(vec.reshape(1, -1))[0][0], pca.transform(vec.reshape(1, -1))[0][1], 'o', color='orange')\n",
    "    plt.annotate(name, (pca.transform(vec.reshape(1, -1))[0][0], pca.transform(vec.reshape(1, -1))[0][1]))\n",
    "    \n",
    "for vec, name in zip([vector7,vector8,vector9,vector10], ['tuesday', 'monday', 'sunday', 'wednesday']):\n",
    "    plt.plot(pca.transform(vec.reshape(1, -1))[0][0], pca.transform(vec.reshape(1, -1))[0][1], 'o', color='black')\n",
    "    plt.annotate(name, (pca.transform(vec.reshape(1, -1))[0][0], pca.transform(vec.reshape(1, -1))[0][1]))\n",
    "    \n",
    "for vec, name in zip([vector11,vector12], ['afghanistan', 'iraq']):\n",
    "    plt.plot(pca.transform(vec.reshape(1, -1))[0][0], pca.transform(vec.reshape(1, -1))[0][1], 'o', color='green')\n",
    "    plt.annotate(name, (pca.transform(vec.reshape(1, -1))[0][0], pca.transform(vec.reshape(1, -1))[0][1]))   \n",
    "\n",
    "for vec, name in zip([vector13,vector14], ['panda', 'monkey']):\n",
    "    plt.plot(pca.transform(vec.reshape(1, -1))[0][0], pca.transform(vec.reshape(1, -1))[0][1], 'o', color='yellow')\n",
    "    plt.annotate(name, (pca.transform(vec.reshape(1, -1))[0][0], pca.transform(vec.reshape(1, -1))[0][1])) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_emb(sent):\n",
    "    sum_vector = np.zeros(100)\n",
    "    for token in sent:\n",
    "        try:\n",
    "            emd = model.wv[token]\n",
    "        except:\n",
    "            emd = np.zeros(100)\n",
    "        sum_vector += emd\n",
    "    return sum_vector/len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentence_emb(all_lemmas[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737ff26f",
   "metadata": {},
   "source": [
    "### Write test data to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80647728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotation(all_lemmas):\n",
    "    \n",
    "    output_filename = f\"assets/annotated_corpus/test/embedding_test.tsv\"\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        \n",
    "        for index, sent in zip(range(len(all_lemmas)),all_lemmas):\n",
    "\n",
    "                    embed = get_sentence_emb(sent)\n",
    "\n",
    "                    f.write(f\"{index}\\t\") \n",
    "\n",
    "                    for emb in embed:\n",
    "                        f.write(f\"{emb}\\t\")\n",
    "\n",
    "                    f.write(\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9fc2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_annotation(all_lemmas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de0ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
